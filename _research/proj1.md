Predictive coding for natural vocal signals in the songbird auditory forebrain

Predictive coding posits that incoming sensory signals are compared to an internal generative model with resulting error signals carried in the responses of single neurons. Empirical support for predictive coding in individual neurons, particularly in the auditory system and for natural stimuli, has proven difficult to observe. Here, we developed a neural network that uses current sensory context to predict future spectral-temporal features in a natural communication signal, birdsong. Using this model, we represent the waveform of any birdsong as either a set of weighted “latent” predictive features evolving in time, or a corresponding error representation that reflects the difference between the predicted and actual song. We then recorded responses of single neurons in caudomedial nidopallium (NCM), caudal mesopallium (CMM) and Field L, analogs of mammalian auditory cortex, in anesthetized European starlings listening to conspecific songs, and computed the linear/non-linear receptive fields for each neuron fit separately to the spectro-temporal, predictive, and error representations of song. Comparisons between the quality of each receptive field model reveal that NCM spiking responses are best modeled by the predictive spectrotemporal features of song, while CMM and Field L responses capture both predictive and error features. Neural activity is selective for carrying information explicitly about prediction and prediction errors, and their preferences vary across the auditory forebrain. We conclude that this provides strong support for the notion that individual neurons in songbirds encode information related to multiple stimulus representations guided by predictive coding simultaneously.
